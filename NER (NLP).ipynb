{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VjGKrN6-hLNd"
      },
      "source": [
        "م، نبدأ بـ 🧠 التعرف على الكيانات المسماة (Named Entity Recognition - NER) باستخدام نموذج متعدد اللغات زي bert-base-multilingual-cased.\n",
        "\n",
        "حنستخدم بيانات جاهزة من datasets مثل WikiAnn، ونبدأ نجهز الكود خطوة خطوة."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4X0LgirRg_8q",
        "outputId": "15681824-ac5e-46b3-b1f0-6c5f2d1f1047"
      },
      "outputs": [],
      "source": [
        "!pip install transformers datasets seqeval\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zQBlZd8ChHKQ"
      },
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 382,
          "referenced_widgets": [
            "f25c689d31094ef5aac475fcd36b9159",
            "5a0e81676e8848be8272381bd1d4416c",
            "1f4152b250dd4ca7a47c5e741faa0770",
            "49b5cd1b4eef448982a8dc78843791a0",
            "e98687ea300c4071b00bf18c54f86ca8",
            "46347b911e1f4478813132781ca9114f",
            "38ccd17b3e29401981be04d424498ac3",
            "80ce9e8d3ed3462abfe12ec1d8ccb6d7",
            "0e43a20713094bdc89c00918befe2c8d",
            "9aee91f3e03146f5af308c8973e9bca7",
            "8369194856c540029805b0553706b81f",
            "568c408b106641b498fe76e380b95459",
            "3fe7fe33bbbe4b76b5bda5872847e91a",
            "b642a5a6e09e46a0b460b0c2b15b69aa",
            "08319b8b043b4e16941c9440fd3cfd81",
            "d145fa9736a740cf87ba76b9a811ee01",
            "c0c3bed5444d4c2b866fe35a2123f13d",
            "44155b8ca8074479b8850ab08306ffe5",
            "307067d936ba487da003343da0ca0883",
            "aabe2f8e85cb4e9fbbb63d4a4aab8637",
            "7cb47a137a4e4265ae23d94f7bc19b44",
            "df8916c35b894badadea7d61c4757096",
            "f60e8571c293492395259992c9a0e61e",
            "a24aacfd8f4a46b5b116744422763d70",
            "20c682b6d12f4037b7884adb5a66346e",
            "8033c5b37f1f4cf8ae09b3bfcd3bf3a6",
            "12aef35c9b824753b01248685b055300",
            "f704c93056344955b3526cc3d0d18d03",
            "9204428530074b1783f9ef3d6abba7e1",
            "a80a1e9d607f4369ac494eb69a61d114",
            "7a6232d7600f48ef983db6e458a8e156",
            "77c55fe51b3f430e8ff0a80ed11060d0",
            "9cfd773745aa4521a2370c3fe6ea4e8c",
            "c70020ef86e247698d4cf5f43e896cd3",
            "914bff3b84b145e9ab83a63fef6391b0",
            "6f2c9889eeae455fab1348564f74b88b",
            "3f5e611694eb4335887b16b232be7081",
            "e8dd4bd97c0c484a8e4f807964e24d1f",
            "ce3fdcd448024e4fad9a25905c63c7f9",
            "01ffc43667104bde8727d3c0b7b89698",
            "cf5b5076a5b740cb80bb7e691a7df992",
            "9777c3019ca64f51a4235f7d08a391d9",
            "21f3616a310247e697abc5de87f9a909",
            "91f21a457e3a406bb59f6049b1363652",
            "5c5c998328e1440a8c702f89fd6707a5",
            "20ee79c481e64e9e89a54a87637da7a5",
            "32d8487dcdd04af98b1db2646ea5dc54",
            "f3bd3f36b79e408da43a4e77d9e82682",
            "e7619e624d5147d6be786cce0ad60a41",
            "ff2c7fa891cb4d1ebfc96335118152e4",
            "015cf18908f245068689c9404a85abba",
            "76696659fbe54741810db0f7f8d67897",
            "38d0e295bc5447b796e87b188aaedb31",
            "740bb6752f124e63812f47a11ff25d62",
            "f2b6e6109da54350b171534ccd9703d7",
            "6871ae05a4a64457baaf0b5b701c007b",
            "a3ae06cf28c44533954bf3428b313b99",
            "1026a0de09a34adaa4e181aed66f79c2",
            "26b4ea4409694b5cb823140a327d559b",
            "3b4e9043fd404c7f8134e2ec8d6c2240",
            "dd866692fb134ec2b4fccf80db3ca831",
            "4adbf056a40942708817fdab34ebd2f6",
            "11c275bfd9a64c3b8ff1bc8e84eaf5d9",
            "ff0750405fb748b1a1f6b4f90fb5308b",
            "86b7bb578b0d4000ae0f06b3d7d7595e",
            "2f80c45da2194060883d885bbb65a45a",
            "6056c37d413341c9a73f995148950b3d",
            "169427c98b8641ddb8bc1c039a6b717f",
            "609a7c86a4c04690987e04cff5d8766a",
            "8b5dbaa463154ee69b4785020bee7f48",
            "3b45471ca13c485b910783a34d100c7e",
            "e374f897ab5a457d92cc6d607919c8be",
            "d23d132ec6044c77a5e133166a7734c3",
            "47c80aadf18c4d179c9ea3c98dd9d298",
            "038876f0acf141d5b2e280782eb46cca",
            "ea6828f3fc764d3293f91bc124bf28bd",
            "a79317ea015846cbaab71964bd812209"
          ]
        },
        "id": "vxMi2uCBhCsl",
        "outputId": "ed336b09-96a6-4afd-8b8e-ba637e2e1539"
      },
      "outputs": [],
      "source": [
        "from datasets import load_dataset\n",
        "\n",
        "# نستخدم بيانات WikiAnn للغة العربية كمثال\n",
        "dataset = load_dataset(\"wikiann\", \"ar\")\n",
        "\n",
        "# نعاين أول مثال من بيانات التدريب\n",
        "print(dataset[\"train\"][0])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 162,
          "referenced_widgets": [
            "dcea3ed0469240d78f06f7741a0e0f81",
            "03396a25415a4ab8806d2345e521f2a1",
            "95aa647e2eb44a60b1f5bbc0ba3fa224",
            "b4aa1c0d68874606bfdd33f54ed25559",
            "a169a97d8e794ad78b8a9b175daee1b0",
            "8b73ac186244477daf7acceb0a1fd43d",
            "edf14fa88dac4ad887cc809d445f569a",
            "10f5f8a3b62d4d669ba74f967f8ab29a",
            "dc71c364d6594c308fd3d4b5a923afb9",
            "56115d032b494d99b97aa4874ce646e1",
            "018dbaefcafd42da9b6988e7ca85df5a",
            "9f99bcfc909f472dac69828fac57931d",
            "b44db584d1164ce0b1afd0f84cbd8f72",
            "4d24a9227c8f4d2885a2c040a7a5fd40",
            "0d53e666f17746af87f5fb50aa1fb65d",
            "f76a633bc20643018018a4bf96fb5397",
            "c6d6b7aab5824428806e12e0e5cf2f7e",
            "e7b4095f1c2945c982acea9a901eb0a6",
            "161fcc5647334c98b98afb1608a6c109",
            "ad258f9f7fa141c4b699279a9cd13e9a",
            "4e90f88bf35b466e8b375415648eed67",
            "2128b261f889450c9e12c1cc910adcd4",
            "1353cd2d439146fcb95bdb3e79984e4d",
            "04d50191af614d7281ca5004461349c8",
            "42b3d49e7f094e97bf24a64e16cabc5d",
            "b5ac55a6cf9749c1a406ac3718b30211",
            "dec571f3937f46eea9cad39e403096b7",
            "079c95dcde2c477e8449baf0579f7a2c",
            "4d8ef40bf1ea44fbb08fb504b863ee29",
            "b96671da407d418c86fec88ec2deb080",
            "d9f82f1653194a4c950723c9adc5c79c",
            "d15dae05b29242ef8296fe261f3eb893",
            "3d0f509aa0054f37ba9d8295049c8f6f",
            "dffccb5f5d454ddc8cd7c9f44534ac53",
            "6e3e9c2543ef42b7821f53f593e83b9c",
            "3af4d06e906b49d383279b5a1c3010fc",
            "addb161455dc4953a93d07ca735287ea",
            "75f161ccb7344494a0b0c2844a487659",
            "dfd3b9f3fc62449d9a30c2d2018d17ed",
            "56e65dd80bbe4adba41e58b03aa06a09",
            "fd66319d489542eda60d49e9c171a65f",
            "050024ef58354cb6aab34270acf7e266",
            "3ecadbf755b749c9876d331f7dc56a65",
            "ad5e0dde88524d69b3ded303c9fd3202"
          ]
        },
        "id": "ubRGeEBLhQWh",
        "outputId": "4bbdd768-9511-4619-a41c-a69a0bbc66ff"
      },
      "outputs": [],
      "source": [
        "from transformers import AutoTokenizer\n",
        "\n",
        "# نحمل Tokenizer الخاص بـ mBERT\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-multilingual-cased\")\n",
        "\n",
        "# مثال: نعاين كيف tokenizer بيشتغل\n",
        "example = dataset[\"train\"][0][\"tokens\"]\n",
        "tokenized_input = tokenizer(example, is_split_into_words=True)\n",
        "print(tokenized_input.tokens())\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yzM3ID-nhUMY",
        "outputId": "44af5812-2736-41e9-a396-97e1ef8c0e68"
      },
      "outputs": [],
      "source": [
        "label_list = dataset[\"train\"].features[\"ner_tags\"].feature.names\n",
        "print(label_list)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "Ly-m3FI1hVve"
      },
      "outputs": [],
      "source": [
        "def tokenize_and_align_labels(examples):\n",
        "    tokenized_inputs = tokenizer(\n",
        "        examples[\"tokens\"],\n",
        "        truncation=True,\n",
        "        is_split_into_words=True\n",
        "    )\n",
        "\n",
        "    labels = []\n",
        "    for i, label in enumerate(examples[\"ner_tags\"]):\n",
        "        word_ids = tokenized_inputs.word_ids(batch_index=i)\n",
        "        previous_word_idx = None\n",
        "        label_ids = []\n",
        "        for word_idx in word_ids:\n",
        "            if word_idx is None:\n",
        "                label_ids.append(-100)\n",
        "            elif word_idx != previous_word_idx:\n",
        "                label_ids.append(label[word_idx])\n",
        "            else:\n",
        "                label_ids.append(label[word_idx] if True else -100)  # OR inside-word strategy\n",
        "            previous_word_idx = word_idx\n",
        "        labels.append(label_ids)\n",
        "\n",
        "    tokenized_inputs[\"labels\"] = labels\n",
        "    return tokenized_inputs\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 113,
          "referenced_widgets": [
            "2ac1585901934cc58ecdfa71037d84be",
            "8952e6a82fd14b61895c16674b4abd2f",
            "07c125834fd74199869642d992b515ed",
            "ccf79b198e4a484a9e98395966f7975c",
            "a70769048f274fd2bdb559d6e0bc4f5d",
            "556018e2274a48ca8dd2a6794af0af28",
            "b3e01857f51042788f5cbc4bf6ab29f3",
            "19270f5af3544b54bf224ac68a484b5f",
            "ceeedc9afde94cb1aa1e71ff20a9b80a",
            "d9c8622f13544a1989f8600a25748db4",
            "50d3ccf55d534f81b6ca01218a03455a",
            "f0d43dc3f43a49539bde610451f6f6cb",
            "8074f756fa4e49bc91b39e5cf0f4e702",
            "0b202d42f03f4333848d9922bd6a78aa",
            "8ef5cd41f1e942e1ad14117b2162f59c",
            "614618ac3e6943828420913da5a2d8cb",
            "be4c9dd1d5be40c791f3630cc5c447d2",
            "9d0e970414ee498c89267618094ca8cb",
            "da271e102ba94cde8ba8659040d8f129",
            "fea9c6247a634f8dba1e31ab9286f08f",
            "68b6a5a375db40c4af48658b044971fc",
            "4ad000c1742f4d1bad6a19c6ffe52e75",
            "24669df7fc994fa8891332e85a634d7e",
            "9255c30c6fc94895aa724e72bcb28f16",
            "ea57287cb2484e6fb24b09ac5f57868d",
            "00087027c79d4fff8f285122fb645d9d",
            "d465e190570d4fa49fe216b1ddbef583",
            "12eb727cbfcb47ba9ef3391ea15b6e8a",
            "962c0c2d25ab426094764ba3e614fc08",
            "be06fd9a532f403389f4c6b431ec129d",
            "11968c8400144027b9d75d371358e571",
            "a2f210607b98494a9e7e4a6d229e1ed5",
            "c35c0cc4573c418caea1c2229898e94a"
          ]
        },
        "id": "OwsiJ9hbhX1w",
        "outputId": "d78eb61f-7c51-4272-e06d-e298fdc9fab1"
      },
      "outputs": [],
      "source": [
        "# تطبيق الدالة على مجموعات البيانات\n",
        "tokenized_datasets = dataset.map(tokenize_and_align_labels, batched=True)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 138,
          "referenced_widgets": [
            "aa95d39d9bf54f489b2b7d835622c347",
            "8dec3de550174b7d911f4d680d5ed828",
            "18bfb35b04bd40479041a5833d8b22b9",
            "859e6209059f4aab9875ee6de929ba97",
            "c8558cb8e54a475f9e9ffd36f062f8a3",
            "4ab80b6b38ab43fe9a485001e93d7110",
            "f4ae0449398a4198a98455626b31eff6",
            "72a952a3b28d4bb499996c9e0dfedaa2",
            "b7fe814975f94ea69f15eb7e377a8dd4",
            "081af4cdee8e43c7acb463f71522436c",
            "df55b2d38ade4f75a0cbad807b63371f"
          ]
        },
        "id": "ol69olLMhbyD",
        "outputId": "6543efc8-2bb7-464d-e3f7-036b1c5f5c44"
      },
      "outputs": [],
      "source": [
        "from transformers import AutoModelForTokenClassification\n",
        "\n",
        "model = AutoModelForTokenClassification.from_pretrained(\n",
        "    \"bert-base-multilingual-cased\",\n",
        "    num_labels=len(label_list)\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "GLOoB2ExhdQP"
      },
      "outputs": [],
      "source": [
        "from transformers import TrainingArguments, Trainer\n",
        "import numpy as np\n",
        "from seqeval.metrics import classification_report, f1_score\n",
        "\n",
        "# دالة لتقييم النموذج\n",
        "def compute_metrics(p):\n",
        "    predictions, labels = p\n",
        "    predictions = np.argmax(predictions, axis=2)\n",
        "\n",
        "    true_predictions = [\n",
        "        [label_list[p] for (p, l) in zip(prediction, label) if l != -100]\n",
        "        for prediction, label in zip(predictions, labels)\n",
        "    ]\n",
        "    true_labels = [\n",
        "        [label_list[l] for (p, l) in zip(prediction, label) if l != -100]\n",
        "        for prediction, label in zip(predictions, labels)\n",
        "    ]\n",
        "\n",
        "    return {\n",
        "        \"f1\": f1_score(true_labels, true_predictions),\n",
        "        \"report\": classification_report(true_labels, true_predictions)\n",
        "    }\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zKXCpRd-hyxw",
        "outputId": "4f660cde-b9a6-44f6-895d-a55da0910066"
      },
      "outputs": [],
      "source": [
        "!pip install -U transformers\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 261,
          "referenced_widgets": [
            "d8964903c5d1445c9d007c1207a44f43",
            "15f549277c214c198048574f7efcbc55",
            "244a13db53ad41afa92d99b86c1d6eda",
            "6a26c403dd8c480688f60d936c711760",
            "dc6f97b974264bfd8ba633887c7013b4",
            "7cdeb9137aa74ad79fa1f2a08bb1bd33",
            "35259ff682e9423b8550ef69fac8b99a",
            "6595ab8a1c9c45eea88c1314a5d67c7f",
            "0490bfcc795d432ea753d5cb07fffdae",
            "c323cee4f8ee49ce8bbaadc0208b0ba2",
            "e7dae71ef9e741c28c23560bfe65e31f",
            "7fbe0157f46b44aaa38e23ead58acc4b",
            "61415b3da759463790666f01f9d5aec1",
            "373aa6f6bae44e2aae696c0e52c3ac53",
            "3f648bf92f9d4189ba2bff445e86b735",
            "7fa6336574de44e99ba4bb305bf41bf2",
            "8596d58c0ef74ff8b6a1733c3990cd06",
            "2621d409f384445098b6c86bee1def41",
            "5be380938c5d4e83998ba5e900fedb45",
            "90177dd819e0482f9a73b878b56d8424",
            "658f7ad9535a4601b4fdb4f2d29c8126",
            "5aecd1fbbbe94e978eb4438ee9d4b1fc",
            "c77fd5c023f54895a0a4d37a89f6e40a",
            "ecaa003419c0442ab7f0f34f1324051e",
            "00d927a11a9b425b997cbbee4394a2c3",
            "37654b40f88a43ceb0862812499fa4ba",
            "72cf05848f484a10964473657b3825cd",
            "5dd91548ee354fba9e7d047dcdd86e83",
            "2a503eb020204755a03e1ce308d5aafb",
            "aefb6e06caa04958921320204ac19bda",
            "a0e06f0f479845328cfe739c33bac7bb",
            "27b43a9d49a04dd89ec0a98d31cc0226",
            "a1271bbb2a5d4ad49b4446e4bf216496"
          ]
        },
        "id": "D92xbLWmhelq",
        "outputId": "cad0c59c-a263-4b7b-b997-9fb8b9a6648f"
      },
      "outputs": [],
      "source": [
        "# ✅ تدريب نموذج NER باللغة العربية باستخدام mBERT - كود كامل (نسخة معدلة)\n",
        "\n",
        "from transformers import AutoTokenizer, AutoModelForTokenClassification, TrainingArguments, Trainer\n",
        "from datasets import load_dataset\n",
        "import numpy as np\n",
        "from seqeval.metrics import f1_score\n",
        "import os\n",
        "\n",
        "# ✅ إيقاف wandb\n",
        "os.environ[\"WANDB_DISABLED\"] = \"true\"\n",
        "\n",
        "# ✅ تحميل البيانات\n",
        "raw_datasets = load_dataset(\"wikiann\", \"ar\")\n",
        "label_list = raw_datasets[\"train\"].features[\"ner_tags\"].feature.names\n",
        "\n",
        "# ✅ تحميل Tokenizer\n",
        "model_checkpoint = \"bert-base-multilingual-cased\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)\n",
        "\n",
        "# ✅ تجهيز البيانات\n",
        "\n",
        "def tokenize_and_align_labels(examples):\n",
        "    tokenized_inputs = tokenizer(\n",
        "        examples[\"tokens\"],\n",
        "        truncation=True,\n",
        "        padding=\"max_length\",\n",
        "        max_length=128,\n",
        "        is_split_into_words=True,\n",
        "    )\n",
        "\n",
        "    labels = []\n",
        "    for i, label in enumerate(examples[\"ner_tags\"]):\n",
        "        word_ids = tokenized_inputs.word_ids(batch_index=i)\n",
        "        previous_word_idx = None\n",
        "        label_ids = []\n",
        "        for word_idx in word_ids:\n",
        "            if word_idx is None:\n",
        "                label_ids.append(-100)\n",
        "            elif word_idx != previous_word_idx:\n",
        "                label_ids.append(label[word_idx])\n",
        "            else:\n",
        "                label_ids.append(-100)\n",
        "            previous_word_idx = word_idx\n",
        "        labels.append(label_ids)\n",
        "\n",
        "    tokenized_inputs[\"labels\"] = labels\n",
        "    return tokenized_inputs\n",
        "\n",
        "# ✅ تطبيق المعالجة\n",
        "encoded_datasets = raw_datasets.map(tokenize_and_align_labels, batched=True)\n",
        "\n",
        "# ✅ تحميل النموذج\n",
        "model = AutoModelForTokenClassification.from_pretrained(\n",
        "    model_checkpoint, num_labels=len(label_list)\n",
        ")\n",
        "\n",
        "# ✅ إعدادات التدريب\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=\"./ner_model\",\n",
        "    learning_rate=2e-5,\n",
        "    per_device_train_batch_size=16,\n",
        "    per_device_eval_batch_size=16,\n",
        "    num_train_epochs=2,\n",
        "    weight_decay=0.01,\n",
        "    logging_dir=\"./logs\",\n",
        "    report_to=\"none\"\n",
        ")\n",
        "\n",
        "# ✅ دالة التقييم\n",
        "\n",
        "def compute_metrics(p):\n",
        "    predictions, labels = p\n",
        "    predictions = np.argmax(predictions, axis=2)\n",
        "    true_predictions = [\n",
        "        [label_list[p] for (p, l) in zip(prediction, label) if l != -100]\n",
        "        for prediction, label in zip(predictions, labels)\n",
        "    ]\n",
        "    true_labels = [\n",
        "        [label_list[l] for (p, l) in zip(prediction, label) if l != -100]\n",
        "        for prediction, label in zip(predictions, labels)\n",
        "    ]\n",
        "    return {\"f1\": f1_score(true_labels, true_predictions)}\n",
        "\n",
        "# ✅ إنشاء المدرب وتدريب النموذج\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=encoded_datasets[\"train\"].shuffle(seed=42).select(range(1000)),\n",
        "    eval_dataset=encoded_datasets[\"validation\"].select(range(200)),\n",
        "    tokenizer=tokenizer,\n",
        "    compute_metrics=compute_metrics\n",
        ")\n",
        "\n",
        "trainer.train()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oF0yYCSXhkkD"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from transformers import AutoModelForTokenClassification, AutoTokenizer\n",
        "from transformers import pipeline\n",
        "\n",
        "# تحميل النموذج المدرب\n",
        "model_path = \"./ner_model/checkpoint-XXX\"  # غيّر XXX برقم آخر Checkpoint عندك\n",
        "ner_tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
        "ner_model = AutoModelForTokenClassification.from_pretrained(model_path)\n",
        "\n",
        "# إنشاء Pipeline للتعرف على الكيانات\n",
        "ner_pipe = pipeline(\"ner\", model=ner_model, tokenizer=ner_tokenizer, aggregation_strategy=\"simple\")\n",
        "\n",
        "# جملة تجريبية\n",
        "text = \"ولد محمد صلاح في قرية نجريج في مصر.\"\n",
        "\n",
        "results = ner_pipe(text)\n",
        "\n",
        "for entity in results:\n",
        "    print(f\"{entity['word']} ({entity['entity_group']}): {entity['score']:.2f}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Cbg4M8yziw4W"
      },
      "outputs": [],
      "source": [
        "from transformers import pipeline\n",
        "\n",
        "# ✅ تحميل بايبلاين NER باستخدام النموذج المدرب\n",
        "ner_pipeline = pipeline(\"ner\", model=model, tokenizer=tokenizer, aggregation_strategy=\"simple\")\n",
        "\n",
        "# ✅ مثال لجملة عربية\n",
        "text = \"ذهب محمد إلى القاهرة لحضور مؤتمر الذكاء الاصطناعي في جامعة عين شمس.\"\n",
        "\n",
        "# ✅ تطبيق النموذج\n",
        "ner_results = ner_pipeline(text)\n",
        "\n",
        "# ✅ عرض النتائج\n",
        "for entity in ner_results:\n",
        "    print(f\"الكلمة: {entity['word']}, التصنيف: {entity['entity_group']}, النتيجة: {entity['score']:.2f}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KdyUuB-Op9L8"
      },
      "outputs": [],
      "source": [
        "# حفظ النموذج والـ tokenizer علشان نقدر نستخدمهم بعدين\n",
        "model.save_pretrained(\"ner_model_arabic\")\n",
        "tokenizer.save_pretrained(\"ner_model_arabic\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ojmC3aNxp_DA"
      },
      "outputs": [],
      "source": [
        "from transformers import AutoTokenizer, AutoModelForTokenClassification\n",
        "\n",
        "model = AutoModelForTokenClassification.from_pretrained(\"ner_model_arabic\")\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"ner_model_arabic\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ioo0tqLUqA0I"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
